{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lyft Competition : Uderstanding the data (한글 번역본)\n",
    "\n",
    "* competition: Lyft 3D Object Detection for Autonomous Vehicles\n",
    "* original kernel: https://www.kaggle.com/tarunpaparaju/lyft-competition-understanding-the-data\n",
    "* share: [Junghyun Park](https://github.com/parkjh688)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "<img src=\"./images/image1.jpg\" alt=\"image1\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='grey'>이 커널에서, 나는 이미지, LiDAR, 포인트클라우드를 포함한 데이터셋의 구성요소의 의미와 직관에 대해 설명할 것 입니다. 이러한 개념에 대한 이론을 살펴본 후, 이 데이터셋에서 정보를 더 쉽게 쿼리할 수 있는 컴팩트 형식으로 패키징할 수 있는 방법을 보여줄 것 입니다. 그리고 마지막으로, 이 데이터를 어떻게 시각화하고 탐구하는지를 matplotlib의 플롯과 그래프를 사용하여 보여줄 것 입니다.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgements\n",
    "\n",
    "* [NuScences DevKit ~ by Lyft](https://github.com/lyft/nuscenes-devkit)\n",
    "* [EDA - 3D Object Detection Challenge ~ by beluga](https://www.kaggle.com/gaborfodor/eda-3d-object-detection-challenge)\n",
    "* [Lyft: EDA, Animations, generating CSVs ~ by xhulu](https://www.kaggle.com/xhlulu/lyft-eda-animations-generating-csvs)\n",
    "* [Lidar - Wikipedia](https://en.wikipedia.org/wiki/Lidar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>이 커널이 흥미롭다면, 좋아요(upvote)를 눌러주세요. 좋아요는 나에게 더 많은 양질의 콘텐츠를 생산하도록 동기를 부여해줍니다 :)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**자율주행 자동차를 시작해봅시다!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 커널의 기술적인 부분들을 시작해보기 전에, 자율주행 자동차에 대한 재미있는 비디오를 하나 봅시다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<center><iframe width=\"700\" height=\"400\" src=\"https://www.youtube.com/embed/tlThdr3O5Qo?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe></center>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<center><iframe width=\"700\" height=\"400\" src=\"https://www.youtube.com/embed/tlThdr3O5Qo?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe></center>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "자동차가 쉽게 교대로, 차선을 바꾸거나, 빨간 신호등에서 정지하는 등의 기능을 할 수 있다는 것을 영상에서 볼 수 있습니다. 이것은 자동차가 이미지나 LiDAR 데이터와 같은 센서의 정보를 사용하여 3D 공간에서 물체를 정확하게 인식할 수 있기 때문에 가능한 일입니다. 이제 이러한 형태의 데이터가 이론적으로 무엇을 의미하는지 살펴본 다음, 커널에서 시각화해볼 것 입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The dataset structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. scene - 주어진 환경에서의 25-45초의 차량 주행으로 구성됨. 각각의 scence 들은 여러 개의 샘플로 구성되어 있음.\n",
    "2. sample - 특정 시간, 특정 인스턴스에 대한 스냅샷. 각 sample에는 object가 어노테이션(annotation) 되어있다.\n",
    "3. sample_data - 차량의 특정 센서에서 수집한 데이터.\n",
    "4. sample_annotation - 우리가 관심 있는 대상의 annotation된 인스턴스.\n",
    "5. instance - 우리가 관찰한 모든 object 인스턴스의 리스트.\n",
    "6. category - object 카테고리(예: 차, 사람)\n",
    "7. attribute - 카테고리가 동일하게 유지되는 동안 변경될 수 있는 인스턴스의 속성.\n",
    "8. visibility - 현재 사용 안함\n",
    "9. sensor - 센서 종류\n",
    "10. calibrated sensor - 특정 차량에서 캘리브레이션(calibration)된 특정 센서.\n",
    "11. ego_pose - 특정 시간대 Ego vehicle의 상태(pose)\n",
    "12. log - 데이터와 관련된 로그\n",
    "13. map - 위에서 본(top-down view) binary semantic mask들"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터의 각 스냅샷은 두 가지 형태의 정보로 구성됩니다: **이미지 데이터와 LiDAR 데이터**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이미지 데이터는 일반적입니다. .jpeg 포맷은 상당히 이해하기 쉽습니다. 각 이미지들은 단순하게 이미지를 형성하는 RGB 컬러 3개의 채널로 구성되어 있습니다:Red (R), Blue (B), and Green (G). 이 컬러 채널들을 겹쳐서 하나의 색이 있는 이미지를 만들어 냅니다. 따라서 이미지들은 4개의 차원을 가진 텐서(Tensor)로 저장되어 있습니다: (batch_size, channels, width, height)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제, 나는 대부분의 사람들은 친숙하지 않을 LiDAR 데이터에 대해 이야기 하려고 합니다. 나는 LiDAR 데이터가 어떻게 수집되고 저장되는지에 대해 설명할 것이고, 이 데이터 포맷이 가지고 있는 의미에 대해서 이야기할 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is LiDAR?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LiDAR(Light Detection and Ranging)는 주변의 정확한 3D 표현을 생성하기 위해 사용되는 방법이며, 이를 달성하기 위해 레이저 빛을 이용합니다. 기본적으로 3D 타겟은 레이저 광선(직선의 빛)으로 비춰지고 반사광은 센서에 의해 수집됩니다. 그리고 빛이 센서에 반사되는 데 필요한 시간을 계산합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다른 센서들은 물체의 다른 부분으로부터 빛을 수집하며, 이 센서에 의해 기록된 시간은 다를 것입니다. 센서가 계산한 시간의 차는 물체의 깊이를 계산하는 데 사용될 수 있습니다. 이 깊이 정보는 이미지의 2D 표현과 결합되어 객체의 정확한 3D 표현을 제공하게 됩니다. 이 과정은 실제 인간의 시각과 비슷합니다. 우리의 두 눈은 2D로 관찰하고, 이 두 개의 정보를 결합해서 3D 지도(depth perception)를 형성합니다. 이것이 바로 인간이 우리 주변의 세상을 이렇게 감지하는 방법입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 기술은, 많은 실제 시나리오에서 3D 표현을 만드는 데 사용됩니다. 예를 들어, 농장에서 씨앗을 뿌리고 잡초를 제거하는 것을 돕기 위해 사용됩니다. 움직이는 로봇은 LiDAR를 사용하여 주변 환경에 대한 3D 지도를 만들고 이 지도를 사용하여 장애물을 피하고 임무를 완수합니다. 이 기술은 고고학에서도 사용됩니다. LiDAR는 2D 유물(artifact) 스캔본의 3D 렌더링하는데 사용됩니다. 이것은 어떤 방법으로도 유물을 발굴할 수 없을 때 유물의 3차원 형상을 정확히 알 수 있게 해줍니다. 마지막으로, LiDAR는 또한 해저와 접근이 불가능한 다른 지형들의 고품질 3D 지도를 만드는 데 사용될 수 있어 지질학자들과 해양학자들에게 매우 유용합니다. 아래는 LiDAR을 사용하여 생성된 해저의 3D 지도입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/image2.jpg\" alt=\"image2\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "물론, 자율주행차는 이 기술을 사용하여 주변의 물체를 3D로 식별하고, 이 물체의 속도와 방향을 추정합니다. 이 포괄적인 3D 지도는 복잡한 환경에서도 차량을 탐색할 수 있도록 상세한 정보를 제공합니다. 아래는 LiDAR를 장착한 드론이 나오는 영상입니다. 위에서 언급한 과정을 이용하여 자동적으로 그 주위에 3D 지도를 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<center><iframe width=\"700\" height=\"400\" src=\"https://www.youtube.com/embed/x7De3tCb3_A?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe></center>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<center><iframe width=\"700\" height=\"400\" src=\"https://www.youtube.com/embed/x7De3tCb3_A?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe></center>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "<br></br>\n",
    "## How does LiDAR work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/image3.gif\" alt=\"image3\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 GIF는 LiDAR가 어떻게 작동하는지 대략적으로 보여주고 있습니다. 기본적으로, 레이저 광선은 레이저에 의해 사방으로 발사됩니다. 레이저 빔은 경로에 있는 물체를 반사하고 반사된 빛은 센서에 의해 수집됩니다. 현재 Flash LiDAR 카메라라고 불리는 특별한 장치가 이러한 센서의 정보를 사용하여 3D 지도를 만드는 데 사용되고 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flash LiDAR Camera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/image4.jpg\" alt=\"image4\" style=\"width: 200px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 이미지에 나타난 장치를 Flash LiDAR 카메라라고 합니다. Flash LiDAR 카메라의 초점면에는 3D 모델을 만들 수 있는 \"depth(깊이)\"와 \"intensity(강도)\" 픽셀의 행과 열이 있습니다. 각각의 픽셀은 레이저 펄스가 목표물에 부딪히고 센서로 돌아오는 데 걸리는 시간과 레이저 펄스에 의해 접촉되는 물체의 깊이, 위치, 반사 강도를 기록합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flash LiDAR 단일 펄스로 시야를 밝히는 단일 광원을 사용합니다. 컬러 대신 거리 사진을 찍는 카메라처럼."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "온보드 조명 소스(onboard source of illumination)는 Flash LiDAR를 활성 센서로 만듭니다. 반환되는 신호는 내장된 알고리즘에 의해 처리되어 센서의 시야 내에서 물체와 지형 형상의 거의 즉각적인 3D 렌더링을 생성하게 됩니다. 레이저 펄스 반복 주파수는 높은 해상도와 정확도를 가진 3D 비디오를 생성하기에 충분합니다. 센서의 프레임률이 높기 때문에 자율주행 등 실시간 시각화가 필요한 다양한 어플리케이션에 유용한 툴입니다. 대상 환경의 3D 고도 메쉬를 즉시 반환함으로써, 무인자동차가 속도 조정, 제동, 조향 등과 관련된 결정을 내릴 수 있게 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이런 형태의 카메라는 자율주행차 상단에 부착되어 있으며, 이 차들은 주행 중 이를 이용해 움직입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "<br></br>\n",
    "이제 LiDAR가 무엇인지, 어떻게 작동하는지 명확하게 알게되었으므로 데이터셋을 바로 시각화해볼 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "<br></br>\n",
    "## Visualizing the data¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lyft_dataset_sdk 및 필요한 라이브러리 설치 하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "우리는 이미지와 LiDAR 데이터를 쉽게 시각화하기 위해 lyft_dataset_sdk 라이브러리가 필요합니다. 간단한 pip install 명령만 있으면 설치는 끝납니다. 또한 인터랙티브(interactive) 플롯을 생성하기 위해 chart_studio 라이브러리를 설치할 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lyft_dataset_sdk\n",
      "  Downloading lyft_dataset_sdk-0.0.8-py2.py3-none-any.whl (32 kB)\n",
      "Collecting scikit-learn>=0.19.2\n",
      "  Using cached scikit_learn-0.22.1-cp37-cp37m-manylinux1_x86_64.whl (7.0 MB)\n",
      "Collecting pytest\n",
      "  Downloading pytest-5.3.5-py3-none-any.whl (235 kB)\n",
      "\u001b[K     |████████████████████████████████| 235 kB 755 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting Pillow>=5.2.0\n",
      "  Downloading Pillow-7.0.0-cp37-cp37m-manylinux1_x86_64.whl (2.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.1 MB 33.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyquaternion>=0.9.5\n",
      "  Downloading pyquaternion-0.9.5-py3-none-any.whl (14 kB)\n",
      "Collecting opencv-python>=3.4.2.17\n",
      "  Downloading opencv_python-4.2.0.32-cp37-cp37m-manylinux1_x86_64.whl (28.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 28.2 MB 152 kB/s eta 0:00:011\n",
      "\u001b[?25hCollecting plotly\n",
      "  Downloading plotly-4.5.0-py2.py3-none-any.whl (7.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.1 MB 104.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting Shapely>=1.6.4.post2\n",
      "  Downloading Shapely-1.7.0-cp37-cp37m-manylinux1_x86_64.whl (1.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.8 MB 37.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm>=4.25.0\n",
      "  Downloading tqdm-4.42.1-py2.py3-none-any.whl (59 kB)\n",
      "\u001b[K     |████████████████████████████████| 59 kB 1.4 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting cachetools>=3.1.0\n",
      "  Using cached cachetools-4.0.0-py3-none-any.whl (10 kB)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.1.3-cp37-cp37m-manylinux1_x86_64.whl (13.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 13.1 MB 933 kB/s eta 0:00:011\n",
      "\u001b[?25hCollecting pandas\n",
      "  Downloading pandas-1.0.0-cp37-cp37m-manylinux1_x86_64.whl (10.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.0 MB 14.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting flake8\n",
      "  Downloading flake8-3.7.9-py2.py3-none-any.whl (69 kB)\n",
      "\u001b[K     |████████████████████████████████| 69 kB 1.7 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting scipy>=1.1.0\n",
      "  Using cached scipy-1.4.1-cp37-cp37m-manylinux1_x86_64.whl (26.1 MB)\n",
      "Collecting fire\n",
      "  Downloading fire-0.2.1.tar.gz (76 kB)\n",
      "\u001b[K     |████████████████████████████████| 76 kB 859 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting numpy>=1.14.5\n",
      "  Downloading numpy-1.18.1-cp37-cp37m-manylinux1_x86_64.whl (20.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 20.1 MB 1.3 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting black\n",
      "  Downloading black-19.10b0-py36-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 1.3 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting joblib>=0.11\n",
      "  Using cached joblib-0.14.1-py2.py3-none-any.whl (294 kB)\n",
      "Collecting py>=1.5.0\n",
      "  Downloading py-1.8.1-py2.py3-none-any.whl (83 kB)\n",
      "\u001b[K     |████████████████████████████████| 83 kB 379 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting packaging\n",
      "  Downloading packaging-20.1-py2.py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: wcwidth in /home/eden/anaconda3/envs/lyft3d/lib/python3.7/site-packages (from pytest->lyft_dataset_sdk) (0.1.8)\n",
      "Collecting more-itertools>=4.0.0\n",
      "  Downloading more_itertools-8.2.0-py3-none-any.whl (43 kB)\n",
      "\u001b[K     |████████████████████████████████| 43 kB 289 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting pluggy<1.0,>=0.12\n",
      "  Downloading pluggy-0.13.1-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /home/eden/anaconda3/envs/lyft3d/lib/python3.7/site-packages (from pytest->lyft_dataset_sdk) (19.3.0)\n",
      "Requirement already satisfied: importlib-metadata>=0.12; python_version < \"3.8\" in /home/eden/anaconda3/envs/lyft3d/lib/python3.7/site-packages (from pytest->lyft_dataset_sdk) (1.5.0)\n",
      "Collecting retrying>=1.3.3\n",
      "  Downloading retrying-1.3.3.tar.gz (10 kB)\n",
      "Requirement already satisfied: six in /home/eden/anaconda3/envs/lyft3d/lib/python3.7/site-packages (from plotly->lyft_dataset_sdk) (1.14.0)\n",
      "Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1\n",
      "  Using cached pyparsing-2.4.6-py2.py3-none-any.whl (67 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/eden/anaconda3/envs/lyft3d/lib/python3.7/site-packages (from matplotlib->lyft_dataset_sdk) (2.8.1)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Using cached kiwisolver-1.1.0-cp37-cp37m-manylinux1_x86_64.whl (90 kB)\n",
      "Collecting cycler>=0.10\n",
      "  Using cached cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n",
      "Collecting pytz>=2017.2\n",
      "  Using cached pytz-2019.3-py2.py3-none-any.whl (509 kB)\n",
      "Requirement already satisfied: entrypoints<0.4.0,>=0.3.0 in /home/eden/anaconda3/envs/lyft3d/lib/python3.7/site-packages (from flake8->lyft_dataset_sdk) (0.3)\n",
      "Collecting pycodestyle<2.6.0,>=2.5.0\n",
      "  Downloading pycodestyle-2.5.0-py2.py3-none-any.whl (51 kB)\n",
      "\u001b[K     |████████████████████████████████| 51 kB 1.1 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting mccabe<0.7.0,>=0.6.0\n",
      "  Downloading mccabe-0.6.1-py2.py3-none-any.whl (8.6 kB)\n",
      "Collecting pyflakes<2.2.0,>=2.1.0\n",
      "  Downloading pyflakes-2.1.1-py2.py3-none-any.whl (59 kB)\n",
      "\u001b[K     |████████████████████████████████| 59 kB 1.4 MB/s  eta 0:00:01\n",
      "\u001b[?25hProcessing /home/eden/.cache/pip/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6/termcolor-1.1.0-cp37-none-any.whl\n",
      "Collecting regex\n",
      "  Downloading regex-2020.1.8-cp37-cp37m-manylinux2010_x86_64.whl (690 kB)\n",
      "\u001b[K     |████████████████████████████████| 690 kB 37.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting appdirs\n",
      "  Downloading appdirs-1.4.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting typed-ast>=1.4.0\n",
      "  Downloading typed_ast-1.4.1-cp37-cp37m-manylinux1_x86_64.whl (737 kB)\n",
      "\u001b[K     |████████████████████████████████| 737 kB 17.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting toml>=0.9.4\n",
      "  Downloading toml-0.10.0-py2.py3-none-any.whl (25 kB)\n",
      "Collecting click>=6.5\n",
      "  Downloading Click-7.0-py2.py3-none-any.whl (81 kB)\n",
      "\u001b[K     |████████████████████████████████| 81 kB 1.7 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting pathspec<1,>=0.6\n",
      "  Downloading pathspec-0.7.0-py2.py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/eden/anaconda3/envs/lyft3d/lib/python3.7/site-packages (from importlib-metadata>=0.12; python_version < \"3.8\"->pytest->lyft_dataset_sdk) (2.1.0)\n",
      "Requirement already satisfied: setuptools in /home/eden/anaconda3/envs/lyft3d/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib->lyft_dataset_sdk) (45.1.0.post20200127)\n",
      "Building wheels for collected packages: fire, retrying\n",
      "  Building wheel for fire (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fire: filename=fire-0.2.1-py2.py3-none-any.whl size=103528 sha256=b765e03da481291beeee12925c3b1977a075bcc387d6046c78704bc1d12749c0\n",
      "  Stored in directory: /home/eden/.cache/pip/wheels/a8/6d/a8/d81d42414b24203fc8beb0452deab949ba62fcfb8c7a49e4b6\n",
      "  Building wheel for retrying (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for retrying: filename=retrying-1.3.3-py3-none-any.whl size=11430 sha256=3a984243906bb46a7906e22643a5f271c1ad9d57b320d81e4f8bfb49a46d7b27\n",
      "  Stored in directory: /home/eden/.cache/pip/wheels/f9/8d/8d/f6af3f7f9eea3553bc2fe6d53e4b287dad18b06a861ac56ddf\n",
      "Successfully built fire retrying\n",
      "Installing collected packages: numpy, scipy, joblib, scikit-learn, py, pyparsing, packaging, more-itertools, pluggy, pytest, Pillow, pyquaternion, opencv-python, retrying, plotly, Shapely, tqdm, cachetools, kiwisolver, cycler, matplotlib, pytz, pandas, pycodestyle, mccabe, pyflakes, flake8, termcolor, fire, regex, appdirs, typed-ast, toml, click, pathspec, black, lyft-dataset-sdk\n",
      "Successfully installed Pillow-7.0.0 Shapely-1.7.0 appdirs-1.4.3 black-19.10b0 cachetools-4.0.0 click-7.0 cycler-0.10.0 fire-0.2.1 flake8-3.7.9 joblib-0.14.1 kiwisolver-1.1.0 lyft-dataset-sdk-0.0.8 matplotlib-3.1.3 mccabe-0.6.1 more-itertools-8.2.0 numpy-1.18.1 opencv-python-4.2.0.32 packaging-20.1 pandas-1.0.0 pathspec-0.7.0 plotly-4.5.0 pluggy-0.13.1 py-1.8.1 pycodestyle-2.5.0 pyflakes-2.1.1 pyparsing-2.4.6 pyquaternion-0.9.5 pytest-5.3.5 pytz-2019.3 regex-2020.1.8 retrying-1.3.3 scikit-learn-0.22.1 scipy-1.4.1 termcolor-1.1.0 toml-0.10.0 tqdm-4.42.1 typed-ast-1.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install lyft_dataset_sdk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 데이터 탐구를 하는데 필요한 다른 라이브러리도 가져와 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import json\n",
    "import math\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import Tuple, List\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics\n",
    "from PIL import Image\n",
    "\n",
    "from matplotlib.axes import Axes\n",
    "from matplotlib import animation, rc\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "from plotly.offline import plot, init_notebook_mode\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "import seaborn as sns\n",
    "from pyquaternion import Quaternion\n",
    "from tqdm import tqdm\n",
    "\n",
    "from lyft_dataset_sdk.utils.map_mask import MapMask\n",
    "from lyft_dataset_sdk.lyftdataset import LyftDataset\n",
    "from lyft_dataset_sdk.utils.geometry_utils import view_points, box_in_image, BoxVisibility\n",
    "from lyft_dataset_sdk.utils.geometry_utils import view_points, transform_matrix\n",
    "from pathlib import Path\n",
    "\n",
    "import struct\n",
    "from abc import ABC, abstractmethod\n",
    "from functools import reduce\n",
    "from typing import Tuple, List, Dict\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "<br></br>\n",
    "**데이터셋 경로(path) 맞추기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../input/3d-object-detection-for-autonomous-vehicles/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**학습 데이터가 저장되어 있는 데이터 프레임 가져오기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File ../input/3d-object-detection-for-autonomous-vehicles/train.csv does not exist: '../input/3d-object-detection-for-autonomous-vehicles/train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-15bebafc8acd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msample_submission\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'sample_submission.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/lyft3d/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/lyft3d/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/lyft3d/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/lyft3d/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/lyft3d/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File ../input/3d-object-detection-for-autonomous-vehicles/train.csv does not exist: '../input/3d-object-detection-for-autonomous-vehicles/train.csv'"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(DATA_PATH + 'train.csv')\n",
    "sample_submission = pd.read_csv(DATA_PATH + 'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
